{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "* The project is about analyze the Immigration data of the US in the year 2016, such as what are the most top 5 heavy duty airport name based on the number of  the immigrant, or the population or average temperature of a specific state ordered by city and race.\n",
    "* As an Data Engineer, our responsibility is to create and operate and ETL that process the Immigration data with other dataset to fullfile the requirement. This project will show the full ETL process that include from the data gathering to data modeling to create the most optimize data model so that Business Analyst or Data Scienctist member can use the data properly.\n",
    "* Note that with only the Immigration data of the US for the year 2016, we can not achieve our goal. Hence,  in this propject, we also  crawl 3 more dataset that can contributed our main goal. The description of all 4 dataset is as the following:\n",
    "\n",
    "1. **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "2. **Airport Code Table**: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "3. **U.S. City Demographic Data**: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "4. **World Temperature Data**: This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "* The step we conduct in this project is as follow:\n",
    "    * Step 1: Scope the Project and Gather Data\n",
    "    * Step 2: Explore and Assess the Data\n",
    "    * Step 3: Define the Data Model\n",
    "    * Step 4: Run ETL to Model the Data\n",
    "    * Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/a0/6cbb1435fa3055307b442e1a87d8498f9c8e2bf741607c777b678be9838f/regex-2022.3.15-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (671kB)\n",
      "\u001b[K    100% |████████████████████████████████| 675kB 15.5MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex\n",
      "Successfully installed regex-2022.3.15\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pyspark as ps\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, col, isnan, when, count, mean, lower, avg, round\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id, dayofweek\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "* For this project we will use 4 source of data:\n",
    "    1. Main data: I-94 Immigration dataset\n",
    "    2. Sub data consist of 3 tables:\n",
    "        \n",
    "        2.1. Airport code dataset\n",
    "        \n",
    "        2.2. US Cities Demographics datset\n",
    "        \n",
    "        2.3. World Temperature dataset\n",
    "\n",
    "#### Describe and Gather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_file_path= '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "imda_df = spark.read.\\\n",
    "format('com.github.saurfang.sas.spark').\\\n",
    "option('header', True).\\\n",
    "load(imda_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total row count: 3096313\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U     None   1979.0  10282016   None   None   \n",
       "1      NaN   ...           Y     None   1991.0       D/S      M   None   \n",
       "2  20691.0   ...        None        M   1961.0  09302016      M   None   \n",
       "3  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "4  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0    None  1.897628e+09   None       B2  \n",
       "1    None  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Total row count: {imda_df.count()}')\n",
    "imda_df.printSchema()\n",
    "imda_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Airport code data\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_Schema= R([\n",
    "    Fld('ident', Str()),\n",
    "    Fld('type', Str()),\n",
    "    Fld('name', Str()),\n",
    "    Fld('elevation_ft', Dbl()),\n",
    "    Fld('continent', Str()),\n",
    "    Fld('iso_country', Str()),\n",
    "    Fld('iso_region', Str()),\n",
    "    Fld('municipality', Str()),\n",
    "    Fld('gps_code', Str()),\n",
    "    Fld('iata_code', Str()),\n",
    "    Fld('local_code', Str()),\n",
    "    Fld('coordinates', Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "schema(ac_Schema).\\\n",
    "load('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: double (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>None</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>None</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NA          US      US-PA      Bensalem      00A      None   \n",
       "1        NA          US      US-KS         Leoti     00AA      None   \n",
       "2        NA          US      US-AK  Anchor Point     00AK      None   \n",
       "3        NA          US      US-AL       Harvest     00AL      None   \n",
       "4        NA          US      US-AR       Newport     None      None   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4       None                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac_df.printSchema()\n",
    "ac_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load US Cities Demographics\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_Schema= R([\n",
    "    Fld('City', Str()),\n",
    "    Fld('State', Str()),\n",
    "    Fld('Median Age', Dbl()),\n",
    "    Fld('Male Population', Int()),\n",
    "    Fld('Female Population', Int()),\n",
    "    Fld('Total Population', Int()),\n",
    "    Fld('Number of Veterans', Int()),\n",
    "    Fld('Foreign-born', Int()),\n",
    "    Fld('Average Household Size', Dbl()),\n",
    "    Fld('State Code', Str()),\n",
    "    Fld('Race', Str()),\n",
    "    Fld('Count', Int())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "options(delimiter= ';').\\\n",
    "schema(uscd_Schema).\\\n",
    "load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.printSchema()\n",
    "uscd_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load World Temperature Data\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_Schema= R([\n",
    "    Fld('dt', Str()),\n",
    "    Fld('AverageTemperature', Dbl()),\n",
    "    Fld('AverageTemperatureUncertainty', Dbl()),\n",
    "    Fld('City', Str()),\n",
    "    Fld('Country', Str()),\n",
    "    Fld('Latitude', Str()),\n",
    "    Fld('Longitude', Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "wtd_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "schema(wtd_Schema).\\\n",
    "load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtd_df.printSchema()\n",
    "wtd_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* First we create a function that check for missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to check for missing values or null values\n",
    "def missnull_check(df):\n",
    "    # Get total rows of dataset\n",
    "    get_all_rows= df.count()\n",
    "\n",
    "    # Count if data is missing or null\n",
    "    missnull_df= df.select(\n",
    "        [count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in df.columns]\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Format data from wide to long\n",
    "    long_missnull_df= pd.melt(missnull_df, var_name= 'headers', value_name= 'miss or null count')\n",
    "    \n",
    "    # Miss or Null ratio\n",
    "    long_missnull_df['data lost ratio']= np.round((long_missnull_df['miss or null count'] / get_all_rows) * 100,3)\n",
    "    \n",
    "    # Sort dataframe\n",
    "    final_df= long_missnull_df.sort_values(by= 'data lost ratio', ascending= False)\n",
    "    \n",
    "    print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess I94 Immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With provided I94_SAS_Labels_Descriptions file we derive the list of valid port and valid cit, valid res, and valid address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read valid port from I94_SAS_Labels_Descriptions\n",
    "get_valid_port= re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_port_dict= {'port_code': [], 'port_name': []}\n",
    "with open('i94validport.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_port.search(line)\n",
    "        valid_port_dict['port_code'].append(match[1])\n",
    "        valid_port_dict['port_name'].append(match[2].split(',')[0].lower())\n",
    "\n",
    "valid_port= list(valid_port_dict['port_code'])\n",
    "\n",
    "# Read valid CIT from I94_SAS_Labels_Descriptions\n",
    "get_valid_cit= re.compile(r'(.*).*\\'(.*)\\'')\n",
    "valid_cit_dict= {'origin_country_code': [], 'origin_country': []}\n",
    "with open('i94validcit.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_cit.search(line)\n",
    "        valid_cit_dict['origin_country_code'].append(float(match[1][:3]))\n",
    "        valid_cit_dict['origin_country'].append(match[2].lower())\n",
    "\n",
    "valid_cit= list(valid_cit_dict['origin_country_code'])  \n",
    "\n",
    "# Read valid Address from I94_SAS_Labels_Descriptions\n",
    "get_valid_addr= re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_addr_dict= {'state_code': [], 'state': []}\n",
    "with open('i94validaddr.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_addr.search(line)\n",
    "        valid_addr_dict['state_code'].append(match[1][:4])\n",
    "        valid_addr_dict['state'].append(match[2].lower())\n",
    "\n",
    "valid_addr= list(valid_addr_dict['state_code'])\n",
    "\n",
    "# Get valid dataframe\n",
    "def data_validation(valid_port, valid_cit, valid_addr, df):\n",
    "    #df_valid_port= df.filter(col('i94port').isin(valid_port))\n",
    "    df_valid= df.filter(\n",
    "        (col('i94port').isin(valid_port)) &\n",
    "        (col('i94cit').isin(valid_cit)) & \n",
    "        (col('i94res').isin(valid_cit)) &\n",
    "        (col('i94addr').isin(valid_addr))\n",
    "    )\n",
    "    return df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Filter out data that only show up in the valid list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_valid= data_validation(valid_port, valid_cit, valid_addr, imda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Since cicid is the primary key for this dataset, it must be unique, hence drop all duplicate cicid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_dropdup= imda_df_valid.drop_duplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Check for missing and null values appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     headers  miss or null count  data lost ratio\n",
      "18   entdepu             2548903           99.987\n",
      "15     occup             2542534           99.737\n",
      "23    insnum             2537920           99.556\n",
      "14  visapost             1448676           56.828\n",
      "22    gender              330090           12.949\n",
      "9    depdate              107102            4.201\n",
      "19   matflag              106237            4.167\n",
      "17   entdepd              106237            4.167\n",
      "24   airline               49107            1.926\n",
      "26     fltno                7985            0.313\n",
      "20   biryear                 123            0.005\n",
      "10    i94bir                 123            0.005\n",
      "21   dtaddto                  78            0.003\n",
      "25    admnum                   0            0.000\n",
      "16   entdepa                   0            0.000\n",
      "0      cicid                   0            0.000\n",
      "1      i94yr                   0            0.000\n",
      "13  dtadfile                   0            0.000\n",
      "12     count                   0            0.000\n",
      "11   i94visa                   0            0.000\n",
      "8    i94addr                   0            0.000\n",
      "7    i94mode                   0            0.000\n",
      "6    arrdate                   0            0.000\n",
      "5    i94port                   0            0.000\n",
      "4     i94res                   0            0.000\n",
      "3     i94cit                   0            0.000\n",
      "2     i94mon                   0            0.000\n",
      "27  visatype                   0            0.000\n"
     ]
    }
   ],
   "source": [
    "missnull_check(imda_df_dropdup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can see that there are 3 columns (entdepu, occup, insnum) with over 98% missing or null values, we will drop these columns out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_columns= ['entdepu', 'occup', 'insnum']\n",
    "\n",
    "imda_df_dropcols= imda_df_dropdup.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* To handle with the 4th and 5th most missing or null column, visa post gender, we can consider replace missing or null values with **'Not mentioned'** to make the data more meaninful, for example, when query, the result will return F (female), M (male), and Not mentioned, that is much more meaning than just F, M, and Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_fillna= imda_df_dropcols.na.fill('Not mentioned', subset= ['gender', 'visapost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* About others collumn that have missing or null values, we perform a drop command to eliminate all of remain missing or null values where all columns is missing or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_final= imda_df_fillna.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Nextup, we convert the ```arrdate``` from SAS date form to readable datetime from and to create a dimension table later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sasdate_converter= udf(\n",
    "    lambda x: dt.datetime(1960, 1, 1) + dt.timedelta(days= int(x)),\n",
    "    TimestampType()\n",
    ")\n",
    "imda_df_final= (\n",
    "    imda_df_final\n",
    "    .withColumn('arrival_date', sasdate_converter('arrdate'))\n",
    "    .withColumn('arrival_year', year(sasdate_converter('arrdate')))\n",
    "    .withColumn('arrival_month', month(sasdate_converter('arrdate')))\n",
    "    .withColumn('arrival_day', dayofmonth(sasdate_converter('arrdate')))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw I94 Immigration dataset size: 3096313\n",
      "I94 Immigration dataset size after validate port, cit, and res: 2549246\n",
      "I94 Immigration dataset size after handling missing and null values: 2549246\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw I94 Immigration dataset size: {imda_df.count()}')\n",
    "print(f'I94 Immigration dataset size after validate port, cit, and res: {imda_df_valid.count()}')\n",
    "print(f'I94 Immigration dataset size after handling missing and null values: {imda_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess Airport Code Dataset\n",
    "* Like the I94 Immigration dataset, we see that the column ident is the primary key, so we must check for its duplicate.\n",
    "* Drop all duplicate in the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_dropdup= ac_df.drop_duplicates(['ident'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* As we inspect the iso_region column, we break down the iso_region format into country and region and keep only the region value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define UDF for spliting iso_region column\n",
    "split_region= udf(\n",
    "    lambda x: x.split('-')[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_region= ac_df_dropdup.withColumn('iso_region', split_region('iso_region'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         headers  miss or null count  data lost ratio\n",
      "9      iata_code               45886           83.315\n",
      "10    local_code               26389           47.915\n",
      "8       gps_code               14045           25.502\n",
      "3   elevation_ft                7006           12.721\n",
      "7   municipality                5676           10.306\n",
      "0          ident                   0            0.000\n",
      "1           type                   0            0.000\n",
      "2           name                   0            0.000\n",
      "4      continent                   0            0.000\n",
      "5    iso_country                   0            0.000\n",
      "6     iso_region                   0            0.000\n",
      "11   coordinates                   0            0.000\n"
     ]
    }
   ],
   "source": [
    "missnull_check(ac_df_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Depend on the missing or null data lost ratio and consider its necessary to this project, these columns: 'iata_code', 'local_mode', 'gps_code' will be drop out this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_columns= ['iata_code', 'local_code', 'gps_code']\n",
    "\n",
    "ac_df_dropcols= ac_df_region.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Next, the since 'elevation_ft' is a numerical data and it has null and missing data, we can fill these data with the mean value of all available value in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "elevation_mean_df= ac_df_dropcols.select(mean(col('elevation_ft')).alias('elevation_mean')).collect()\n",
    "\n",
    "elevation_mean= elevation_mean_df[0]['elevation_mean']\n",
    "    \n",
    "ac_elevation_fillna= ac_df_dropcols.na.fill(elevation_mean, subset= ['elevation_ft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we drop all rows that all columns are missing or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_final= ac_elevation_fillna.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Airport code dataset size: 55075\n",
      "Airport code dataset size after handling missing and null values: 55075\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw Airport code dataset size: {ac_df.count()}')\n",
    "print(f'Airport code dataset size after handling missing and null values: {ac_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess US Cities Demographics\n",
    "* With this dataset, we notice that there are 3 columns that can be considered to make a unique key, that is City. State and Race, we call it key column\n",
    "* First, we count all recorded in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we check for duplicated data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We see that there are not any duplicated values in the dataset, thus every record is bounded to the unique key (primary key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With each key column, we check distinct values for this columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.select('City').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.select('State').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.select('Race').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With three key alone, we can not cover for all recorded in the dataset, we will considered combine these columns to create a composite key. In this case, we will combine all 3 keys, since we know that City is the main measurement here, and only State and Race can not cover all the record, so combine 3 column can create a composite key to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uscd_df.select(['City', 'State', 'Race']).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can see that 2891 unique records is match with the non-duplicated raw dataset --> We can conclude primary key for this dataset will be State-City-Race.\n",
    "* Next up, we will check for the missing or null value appear in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   headers  miss or null count  data lost ratio\n",
      "8   Average Household Size                  16            0.553\n",
      "6       Number of Veterans                  13            0.450\n",
      "7             Foreign-born                  13            0.450\n",
      "3          Male Population                   3            0.104\n",
      "4        Female Population                   3            0.104\n",
      "0                     City                   0            0.000\n",
      "1                    State                   0            0.000\n",
      "2               Median Age                   0            0.000\n",
      "5         Total Population                   0            0.000\n",
      "9               State Code                   0            0.000\n",
      "10                    Race                   0            0.000\n",
      "11                   Count                   0            0.000\n"
     ]
    }
   ],
   "source": [
    "missnull_check(uscd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The data loss ratio is below 0.5%, we will drop all row that included missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df_final= uscd_df.dropna(\n",
    "    subset=[\n",
    "        'Average Household Size', \n",
    "        'Number of Veterans', \n",
    "        'Foreign-born', \n",
    "        'Male Population',\n",
    "        'Female Population'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df_final= uscd_df_final.withColumn('City', lower(col('City')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw US Cities Demographics dataset size: 2891\n",
      "US Cities Demographics dataset size after handling missing and null values: 2875\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw US Cities Demographics dataset size: {uscd_df.count()}')\n",
    "print(f'US Cities Demographics dataset size after handling missing and null values: {uscd_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess World Temperature Data\n",
    "* Before deep dive into this dataset, we need to get back to the I94 Immigration dataset.\n",
    "* Get the year appear in the I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| i94yr|\n",
      "+------+\n",
      "|2016.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imda_df_final.select('i94yr').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We see that the I94 Immigration dataset only show the data for the year 2016. Thus, for this World Temperature dataset, we will filter only data for year 2016 only.\n",
    "* First, we break the dt column down into small portion like year, month, and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_ymd= (\n",
    "    wtd_df\n",
    "    .withColumn('record_year', year('dt'))\n",
    "    .withColumn('record_month', month('dt'))\n",
    "    .withColumn('record_day', dayofmonth('dt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|record_year|\n",
      "+-----------+\n",
      "|       2013|\n",
      "|       2012|\n",
      "|       2011|\n",
      "|       2010|\n",
      "|       2009|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wtd_df_ymd.select('record_year').\\\n",
    "distinct().\\\n",
    "orderBy('record_year', ascending= False).\\\n",
    "limit(5).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* As we can see, there is no record for the year 2016. The most recent year is 2013\n",
    "* Also, according to this [question](https://knowledge.udacity.com/questions/293780), we just can select most recent year in the dataset that as close as the year 2016\n",
    "* Thus, about this dataset, first we need to take only data where year from 2011 to 2013.\n",
    "* Also, as we aim to merge this dataset with the US Demographic dataset, so we only need temperature data from the US only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_filter= wtd_df_ymd.filter((col('record_year') >= 2011) & (col('Country')== 'United States')).drop('Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we count all the record for this filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8481"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtd_df_filter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* then, with the filtered data, we check for its duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8481"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtd_df_filter.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can confirm that there is no duplicated data appear in our dataset\n",
    "* Also, about this dataset, we can determine that two column dt and columnn City will the composite key to this dataset\n",
    "* Next, we check for the missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         headers  miss or null count  data lost ratio\n",
      "1             AverageTemperature                   1            0.012\n",
      "2  AverageTemperatureUncertainty                   1            0.012\n",
      "0                             dt                   0            0.000\n",
      "3                           City                   0            0.000\n",
      "4                       Latitude                   0            0.000\n",
      "5                      Longitude                   0            0.000\n",
      "6                    record_year                   0            0.000\n",
      "7                   record_month                   0            0.000\n",
      "8                     record_day                   0            0.000\n"
     ]
    }
   ],
   "source": [
    "missnull_check(wtd_df_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* This dataset is about temperature, and since there is only 1 record got missing or null values, it becames meaningless to use this record. Hence, we will drop this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_dropna= wtd_df_filter.dropna(\n",
    "    subset=[\n",
    "        'AverageTemperature', \n",
    "        'AverageTemperatureUncertainty'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we lower all the city in the ```City``` column and calculate the mean temperature for both AverageTemperature and AverageTemperatureUncertainty columns by ```City```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_final= wtd_df_dropna.select([lower(col('City')).alias('us_city'), 'AverageTemperature', 'AverageTemperatureUncertainty'])\n",
    "wtd_df_final= wtd_df_final.groupBy('us_city').agg(round(avg('AverageTemperature'), 2).alias('avg_temperature'), round(avg('AverageTemperatureUncertainty'), 2).alias('avg_temperature_uncertainty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Word Temperature dataset size: 8599212\n",
      "Word Temperature dataset size after filtering, handling missing and null values: 8480\n",
      "Word Temperature dataset size after aggregated: 248\n"
     ]
    }
   ],
   "source": [
    "print(f'Raw Word Temperature dataset size: {wtd_df.count()}')\n",
    "print(f'Word Temperature dataset size after filtering, handling missing and null values: {wtd_df_dropna.count()}')\n",
    "print(f'Word Temperature dataset size after aggregated: {wtd_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### This is the end for Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "* For this project, the Data Model is designed as the following 1 fact table (**fact_immigration**) and 4 dimension tables (**dim_uscd**, **dim_aircode**, **dim_arrivaldate**)\n",
    "\n",
    "    ![Data model](data-model.png)\n",
    "\n",
    "\n",
    "1. **Fact Table:** **fact_immigration** This table data is extracted fromt the I94 Immigration dataset\n",
    "| fact_immigration |\n",
    "| --------- |\n",
    "| cic_id (PK) |\n",
    "| entry_year |\n",
    "| entry_month |\n",
    "| port_code |\n",
    "| arrival_sas_date |\n",
    "| entry_mode |\n",
    "| address_state_code |\n",
    "| origin_country_code |\n",
    "| origin_residential_country_code |\n",
    "| visa_mode |\n",
    "| count |\n",
    "| birth_year |\n",
    "| gender |\n",
    "| airline |\n",
    "| addmission_number |\n",
    "| flight_number |\n",
    "| visatype |\n",
    "\n",
    "2. **Dimension tables:**\n",
    "\n",
    "* **Aircode table**\n",
    "\n",
    "| dim_aircode |\n",
    "| --------- |\n",
    "| indent (PK) |\n",
    "| type |\n",
    "| name |\n",
    "| elevation_ft |\n",
    "| iso_country |\n",
    "| iso_region (FK) |\n",
    "| municipality |\n",
    "| coordinates |\n",
    "\n",
    "* **US Demographic table:** This table consists of two dataset: The US Demographics dataset and the World Temperature dataset\n",
    "\n",
    "| dim_uscd |\n",
    "| --------- |\n",
    "| city (CK) |\n",
    "| state (CK) |\n",
    "| race (CK) |\n",
    "| state_code (FK) |\n",
    "| median_age |\n",
    "| male_population |\n",
    "| female_population |\n",
    "| total_population |\n",
    "| number_veterans |\n",
    "| foreign_born |\n",
    "| avg_household_size |\n",
    "| count |\n",
    "| avg_temperature |\n",
    "| avg_temperature_uncertainty |\n",
    "| latitude |\n",
    "| longitude |\n",
    "\n",
    "* **Arrival date table:**\n",
    "\n",
    "| dim_arrivaldate |\n",
    "| --------- |\n",
    "| arrival_sas_date (PK) |\n",
    "| arrival_date |\n",
    "| arrival_year |\n",
    "| arrival_month |\n",
    "| arrival_day |\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The Data Pipeline step is as follow:\n",
    "1. Load 4 dataset from source: I94 Immigration dataset, Aircode dataset, US cities demographics dataset, and World Temperature dataset\n",
    "2. Clean the dataset and perform data aggregation to World Temperature dataset by attribute ```City``` (As perform in Step 2)\n",
    "3. Create fact table ```fact_immigration``` from I94 Immigration dataset and write to parquet file partitioned by ```entry_year``` and ```entry_month```\n",
    "4. Create dimension table ```dim_aircode``` from Aircode dataset and write to parquet file.\n",
    "5. Create dimension table ```dim_uscd``` from combined dataset US Citites Demographics and World Temperature, then write to partitioned by ```state```, ```city```, and ```race```\n",
    "6. Create two dimension tables dim ```dim_arrivaldate``` from I94 Immigration dataset's ```arrival_date```, then write to parquet file partitioned by year and month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "0. First, we define the output data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_path= '/home/workspace/output_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Create **fact_immigration** table from I-94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Rename and re-define columns in the dataset\n",
    "imda_df_final= imda_df_final.select(\n",
    "    col('cicid').cast(Int()).alias('cic_id'),\n",
    "    col('i94yr').cast(Int()).alias('entry_year'),\n",
    "    col('i94mon').cast(Int()).alias('entry_month'),\n",
    "    col('i94cit').cast(Int()).alias('origin_country_code'),\n",
    "    col('i94res').cast(Int()).alias('origin_resident_country_code'),\n",
    "    col('i94port').alias('port_code'),\n",
    "    col('arrdate').cast(Int()).alias('arrival_sas_date'),\n",
    "    col('i94mode').cast(Int()).alias('entry_mode'),\n",
    "    col('i94addr').alias('address_state_code'),\n",
    "    col('i94visa').cast(Int()).alias('visa_mode'),\n",
    "    col('biryear').cast(Int()).alias('birth_year'),\n",
    "    col('admnum').cast(Int()).alias('addmission_number'),\n",
    "    col('fltno').cast(Int()).alias('flight_number'),\n",
    "    col('count').cast(Int()).alias('count'),\n",
    "    'arrival_date',\n",
    "    'arrival_year',\n",
    "    'arrival_month',\n",
    "    'arrival_day',\n",
    "    'gender',\n",
    "    'airline',\n",
    "    'visatype'\n",
    ")\n",
    "\n",
    "fact_immigration_table= imda_df_final.select(\n",
    "    'cic_id',\n",
    "    'entry_year',\n",
    "    'entry_month',\n",
    "    'port_code',\n",
    "    'arrival_sas_date',\n",
    "    'entry_mode',\n",
    "    'address_state_code',\n",
    "    'origin_country_code',\n",
    "    'origin_resident_country_code',\n",
    "    'visa_mode',\n",
    "    'count',\n",
    "    'birth_year',\n",
    "    'gender',\n",
    "    'airline',\n",
    "    'addmission_number',\n",
    "    'flight_number',\n",
    "    'visatype'\n",
    ")\n",
    "\n",
    "fact_immigration_table.createOrReplaceTempView('fact_immigration_temp')\n",
    "\n",
    "fact_immigration_table.\\\n",
    "write.\\\n",
    "mode('overwrite').\\\n",
    "partitionBy('entry_year', 'entry_month').\\\n",
    "parquet(path= os.path.join(output_path, 'fact_immigration.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. Create **dim_aircode** table from Airport code dataset schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_aircode_table= ac_df_final.filter(col('iso_country') == 'US')\n",
    "dim_aircode_table.createOrReplaceTempView('dim_aircode_temp')\n",
    "dim_aircode_table.write.mode('overwrite').parquet(path= os.path.join(output_path, 'dim_aircode.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Create **dim_uscd** table by using US Cities Demographics dataset JOIN World Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Rename columns in the dataset\n",
    "uscd_df_final= uscd_df_final.select(\n",
    "    col('City').alias('city'),\n",
    "    col('State').alias('state'),\n",
    "    col('Race').alias('race'),\n",
    "    col('State Code').alias('state_code'),\n",
    "    col('Median Age').alias('median_age'),\n",
    "    col('Male Population').alias('male_population'),\n",
    "    col('Female Population').alias('female_population'),\n",
    "    col('Total Population').alias('total_population'),\n",
    "    col('Number of Veterans').alias('number_veterans'),\n",
    "    col('Foreign-born').alias('foreign_born'),\n",
    "    col('Average Household Size').alias('avg_household_size'),\n",
    "    col('Count').alias('count')\n",
    ")\n",
    "\n",
    "dim_uscd_table= uscd_df_final.join(wtd_df_final, uscd_df_final.city == wtd_df_final.us_city, 'left')\n",
    "\n",
    "dim_uscd_table.createOrReplaceTempView('dim_uscd_temp')\n",
    "\n",
    "dim_uscd_table.write.\\\n",
    "mode('overwrite').\\\n",
    "partitionBy('state', 'city', 'race').\\\n",
    "parquet(path= os.path.join(output_path, 'dim_uscd.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. Create **dim_arrivaldate** tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_arrivaldate= imda_df_final.select(\n",
    "    'arrival_sas_date',\n",
    "    'arrival_date',\n",
    "    'arrival_year',\n",
    "    'arrival_month',\n",
    "    'arrival_day'\n",
    ").distinct()\n",
    "\n",
    "dim_arrivaldate.createOrReplaceTempView('dim_arrivaldate_temp')\n",
    "\n",
    "dim_arrivaldate.write.\\\n",
    "mode('overwrite').\\\n",
    "partitionBy('arrival_year', 'arrival_month').\\\n",
    "parquet(path= os.path.join(output_path, 'dim_arrivaldate.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. SQL check:  Query an SQL confirm that data model is right formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Check all airport that where immigrant stay in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct\n",
    "        a.name as airport_name,\n",
    "        count(i.cic_id) as immigrant_number\n",
    "    from\n",
    "        dim_aircode_temp as a\n",
    "    inner join\n",
    "        fact_immigration_temp as i\n",
    "    on\n",
    "        i.address_state_code == a.iso_region\n",
    "    group by\n",
    "        a.name\n",
    "    order by\n",
    "        immigrant_number\n",
    "    limit\n",
    "        5\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Check the total population and average temperature in each state where immigrant move in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct\n",
    "        i.address_state_code as state code,\n",
    "        u.state as state,\n",
    "        u.city as city,\n",
    "        u.race as race,\n",
    "        u.total_population as total population,\n",
    "        u.avg_temperature as average temperature\n",
    "    from\n",
    "        fact_immigration_temp as i\n",
    "    inner join\n",
    "        dim_uscd_temp as u\n",
    "    on\n",
    "        i.address_state_code == u.state_code\n",
    "    order by\n",
    "        total population\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. Check for null in primary key of each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_qualiity_check(quality_check_list, sqlcontext):\n",
    "    \n",
    "    failed_test_case= 0\n",
    "    \n",
    "    for sql in quality_check_list:\n",
    "        query= sql.get('check_sql')\n",
    "        expected_result= sql.get('expected_result')\n",
    "        \n",
    "        records= sqlcontext.sql(f'{query}').first()['count']\n",
    "        \n",
    "        if expected_result != records:\n",
    "            print(f'Failed at query {query}')\n",
    "            failed_test_case+= 1\n",
    "        \n",
    "    if failed_test_case > 0:\n",
    "        print(f'{failed_test_case} table(s) test failed')\n",
    "    else:\n",
    "        print('All tables has passed the data quality check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables has passed the data quality check\n"
     ]
    }
   ],
   "source": [
    "sqlcontext= SQLContext(spark)\n",
    "\n",
    "quality_check_list= [\n",
    "    {'check_sql': 'select count(*) as count from fact_immigration_temp where cic_id is null', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_aircode_temp where ident is null', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_uscd_temp where (city is null and state is null and race is null)', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_arrivaldate_temp where arrival_sas_date is null', 'expected_result': 0}\n",
    "]\n",
    "\n",
    "data_qualiity_check(quality_check_list, sqlcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "1. fact_immigration\n",
    "    * ```cic_id```: entry id recorded immigrate into the US, stored as **Primary Key**\n",
    "    * ```entry_year```: entry year recorded immigrate into the US\n",
    "    * ```entry_month```: entry month recorded immigrate into the US\n",
    "    * ```port_code```: entry_port_code\n",
    "    * ```arrival_sas_date```: arrival data into the US, SAS format, stored as **Foreign Key**, link to ```dim_arrivaldate```\n",
    "    * ```entry_mode```: entry type when immigrate into the US (1 for Air, 2 for Sea, 3 for Land and 9 for Not Reported)\n",
    "    * ```address_state_code```: Immigrant address state code after immigrated into the US, stored as **Foreign Key**, link to ```dim_aircode``` and ```dim_uscd```\n",
    "    * ```origin_country_code```: Immigrant original country\n",
    "    * ```origin_resident_country_code```: Immigrant orginal residental country\n",
    "    * ```visa_mode```: Immigrant visa mode (1 for Business, 2 for Pleasure, and 3 for Student)\n",
    "    * ```count```: Immigrant number for one record\n",
    "    * ```birth_year```: Immigrant birth year (who fill the record)\n",
    "    * ```gender```: Immigrant gender (who fill the record)\n",
    "    * ```airline```: Airline that immigrant used when immigrate into the US\n",
    "    * ```addmission_number```: Record Admission Number\n",
    "    * ```flight_number```: Flight number of the Airline\n",
    "    * ```visatype```: Class of admission legally admitting the non-immigrant to temporarily stay in US\n",
    "    \n",
    "2. Dimension tables:\n",
    "* dim_aircode\n",
    "    * ```ident```: Airport id, stored as **Primary Key**\n",
    "    * ```type```: Airport type\n",
    "    * ```name```: Airport name\n",
    "    * ```elevation_ft```: Airport elevation\n",
    "    * ```iso_country```: Country that Airport located, ISO format \n",
    "    * ```iso_region```: Region (depend on country) that Airport located, ISO format, stored as **Foreign Key**, link to ```fact_immigration```\n",
    "    * ```municipality```: City, Town (depend on country, region) that Airport located\n",
    "    * ```coordinates```:  Airport coordinate\n",
    "    \n",
    "* dim_uscd\n",
    "    * ```state```: US state, stored as **Composite Key**\n",
    "    * ```city```: US city, stored as **Composite Key**\n",
    "    * ```race```: people race, stored as **Composite Key**\n",
    "    * ```state_code```: US state code, stored as **Foreign Key**, link to ```fact_immigration```\n",
    "    * ```median_age```: Population median age\n",
    "    * ```male_population```: Population of male\n",
    "    * ```female_population```: Population of female\n",
    "    * ```total_population```: Population for total gender\n",
    "    * ```number_veterans```:  Number of veterans in the population\n",
    "    * ```foreign_born```: Number of people not bornt in US\n",
    "    * ```avg_household_size```: Average Household size\n",
    "    * ```count```: Count of household\n",
    "    * ```avg_temperature```: Average temperature for each city, based on World Temperature dataset\n",
    "    * ```avg_temperature_uncertainty```: Average temperature uncertainty for each city, based on World Temperature dataset\n",
    "    * ```latitude```: City latitude\n",
    "    * ```longitude```: City longitude\n",
    "    \n",
    "* dim_arrivaldate\n",
    "    * ```arrival_sas_date```: Arrival date, SAS format, stored as **Primary Key**\n",
    "    * ```arrival_date```: Arrival date, Gregorian format\n",
    "    * ```arrival_year```: Arrival year\n",
    "    * ```arrival_month```: Arrival month\n",
    "    * ```arrival_day```: Arrival day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * For this project, we use **Apache Spark** since it suitable for batch processing and also with spark we can deal with large amount of data.\n",
    "    * Also, for the data model, we have designed the model as the **Star schema model**. The reasion behind this choice is that Star schema is the fundamental schema among the data mart schema and it is simplest. This schema is widely used to develop or build a data warehouse and dimensional data marts. It includes one or more fact tables indexing any number of dimensional tables. So that, not only the Data Engineer, but for Business Analyst or Data Scienctis, who is the user of the output of the ETL process, can catchup everything going on and easily comprehend the situation just only by looking at the data model.\n",
    "    \n",
    "* Propose how often the data should be updated and why.\n",
    "    * For this data, as fact table is immigration data, it is considered to update the data monthly since it is not necessary to see a day-by-day arrival/departure immigrants. This one-month update step will help the server can get a enough amount of data for each dataset to process.\n",
    "    * One more crucial factor is that when consider run an update any data, we must consider the update schedule of the source data, based on this article [here](https://www.trade.gov/i-94-arrivals-program), the source I94 Immigration data is update monthly, hence it is reccomended to update the data monthly\n",
    "    * Still, depend on the requirement of end-user, the update step is considered in order to fullfil both technical requirement and end-user requirement.\n",
    "    \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * We will consider to upload our pipeline to a better infrastructure, such as Amazon Web Service. The AWS infrastructure with S3 service will handle a vast amount of data in the case of our data get bigger. Redshift will create a cluster machine that can speed up the data processing.\n",
    "     \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * To update the data automatically, we will consider 2 options: AWS service Event Bridge or Apache Airflow. \n",
    "         * Event Bridge keep our update service always on time by trigger an update function. An update function can be a Python script for updating the data that deployed to a Lambda function or a Elastic Container Service (ECS).\n",
    "         * Apache Airflow is an service from Apache that automate an data pipeline through a Direct Acyclic Graph.\n",
    "     \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * With AWS, we can set an amount (can be over 100) user and its permission to any service by IAM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
