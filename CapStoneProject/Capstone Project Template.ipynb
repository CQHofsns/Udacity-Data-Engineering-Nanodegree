{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "* The project is about analyze the Immigration data of the US in the year 2016, such as what state does the immigrant live in have the most airports, or the population or average temperature of a specific state ordered by city and race.\n",
    "* As an Data Engineer, our responsibility is to create and operate and ETL that process the Immigration data with other dataset to fullfile the requirement. This project will show the full ETL process that include from the data gathering to data modeling to create the most optimize data model so that Business Analyst or Data Scienctist member can use the data properly.\n",
    "* Note that with only the Immigration data of the US for the year 2016, we can not achieve our goal. Hence, in this propject, we also crawl 3 more dataset that can contributed our main goal. The description of all 4 dataset is as the following:\n",
    "\n",
    "1. **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "2. **Airport Code Table**: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "3. **U.S. City Demographic Data**: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "4. **World Temperature Data**: This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "* The step we conduct in this project is as follow:\n",
    "    * Step 1: Scope the Project and Gather Data\n",
    "    * Step 2: Explore and Assess the Data\n",
    "    * Step 3: Define the Data Model\n",
    "    * Step 4: Run ETL to Model the Data\n",
    "    * Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (2022.3.15)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, col, isnan, when, count, mean, lower, avg, round\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, monotonically_increasing_id, dayofweek\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, TimestampType\n",
    "\n",
    "import etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "* For this project we will use 4 source of data:\n",
    "    1. Main data: I-94 Immigration dataset\n",
    "    2. Sub data consist of 3 tables:\n",
    "        \n",
    "        2.1. Airport code dataset\n",
    "        \n",
    "        2.2. US Cities Demographics datset\n",
    "        \n",
    "        2.3. World Temperature dataset\n",
    "\n",
    "#### Describe and Gather Data etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark():\n",
    "    spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark= create_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The dataset loading step will be package in one fucntion in the ```load_dataset.py``` script. Still, in order to give reader a better understanding about this step, we will break each function in the ```load_dataset.py``` into 4 parts, each part for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Immigration Data (function: load_immigration_dataset())\n",
    "* First we load the data file setup the file path and using spark to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_file_path= '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "imda_df = spark.read.\\\n",
    "format('com.github.saurfang.sas.spark').\\\n",
    "option('header', True).\\\n",
    "load(imda_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f'Total row count: {imda_df.count()}')\n",
    "imda_df.printSchema()\n",
    "imda_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load Airport code data (function: load_aircode_dataset())\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_Schema= R([\n",
    "    Fld('ident', Str()),\n",
    "    Fld('type', Str()),\n",
    "    Fld('name', Str()),\n",
    "    Fld('elevation_ft', Dbl()),\n",
    "    Fld('continent', Str()),\n",
    "    Fld('iso_country', Str()),\n",
    "    Fld('iso_region', Str()),\n",
    "    Fld('municipality', Str()),\n",
    "    Fld('gps_code', Str()),\n",
    "    Fld('iata_code', Str()),\n",
    "    Fld('local_code', Str()),\n",
    "    Fld('coordinates', Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "schema(ac_Schema).\\\n",
    "load('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df.printSchema()\n",
    "ac_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load US Cities Demographics (function: load_us_cities_demographic_dataset())\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_Schema= R([\n",
    "    Fld('City', Str()),\n",
    "    Fld('State', Str()),\n",
    "    Fld('Median Age', Dbl()),\n",
    "    Fld('Male Population', Int()),\n",
    "    Fld('Female Population', Int()),\n",
    "    Fld('Total Population', Int()),\n",
    "    Fld('Number of Veterans', Int()),\n",
    "    Fld('Foreign-born', Int()),\n",
    "    Fld('Average Household Size', Dbl()),\n",
    "    Fld('State Code', Str()),\n",
    "    Fld('Race', Str()),\n",
    "    Fld('Count', Int())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "options(delimiter= ';').\\\n",
    "schema(uscd_Schema).\\\n",
    "load('us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.printSchema()\n",
    "uscd_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load World Temperature Data (function: load_world_temperature_dataset())\n",
    "* First we re-define the data type for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_Schema= R([\n",
    "    Fld('dt', Str()),\n",
    "    Fld('AverageTemperature', Dbl()),\n",
    "    Fld('AverageTemperatureUncertainty', Dbl()),\n",
    "    Fld('City', Str()),\n",
    "    Fld('Country', Str()),\n",
    "    Fld('Latitude', Str()),\n",
    "    Fld('Longitude', Str())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "wtd_df= spark.read.\\\n",
    "format('csv').\\\n",
    "option('header', True).\\\n",
    "schema(wtd_Schema).\\\n",
    "load(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we check the schema and the first 5 rows of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df.printSchema()\n",
    "wtd_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "\n",
    "* **Important Note**: Although that we already define 4 function that can handle this step for us in the ```preprocess.py``` script, still, to give the reader to comprehend how and why we have choose these preprocessing for each dataset in project, we will go and explain through each function in the script one-by-one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* First we create a function that check for missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to check for missing values or null values\n",
    "def missnull_check(df):\n",
    "    # Get total rows of dataset\n",
    "    get_all_rows= df.count()\n",
    "\n",
    "    # Count if data is missing or null\n",
    "    missnull_df= df.select(\n",
    "        [count(when(isnan(column) | col(column).isNull(), column)).alias(column) for column in df.columns]\n",
    "    ).toPandas()\n",
    "    \n",
    "    # Format data from wide to long\n",
    "    long_missnull_df= pd.melt(missnull_df, var_name= 'headers', value_name= 'miss or null count')\n",
    "    \n",
    "    # Miss or Null ratio\n",
    "    long_missnull_df['data lost ratio']= np.round((long_missnull_df['miss or null count'] / get_all_rows) * 100,3)\n",
    "    \n",
    "    # Sort dataframe\n",
    "    final_df= long_missnull_df.sort_values(by= 'data lost ratio', ascending= False)\n",
    "    \n",
    "    print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess I94 Immigration (function: preprocess_immigration_dataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With provided I94_SAS_Labels_Descriptions file we derive the list of valid port and valid cit, valid res, and valid address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read valid port from I94_SAS_Labels_Descriptions\n",
    "get_valid_port= re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_port_dict= {'port_code': [], 'port_name': []}\n",
    "with open('i94validport.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_port.search(line)\n",
    "        valid_port_dict['port_code'].append(match[1])\n",
    "        valid_port_dict['port_name'].append(match[2].split(',')[0].lower())\n",
    "\n",
    "valid_port= list(valid_port_dict['port_code'])\n",
    "\n",
    "# Read valid CIT from I94_SAS_Labels_Descriptions\n",
    "get_valid_cit= re.compile(r'(.*).*\\'(.*)\\'')\n",
    "valid_cit_dict= {'origin_country_code': [], 'origin_country': []}\n",
    "with open('i94validcit.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_cit.search(line)\n",
    "        valid_cit_dict['origin_country_code'].append(float(match[1][:3]))\n",
    "        valid_cit_dict['origin_country'].append(match[2].lower())\n",
    "\n",
    "valid_cit= list(valid_cit_dict['origin_country_code'])  \n",
    "\n",
    "# Read valid Address from I94_SAS_Labels_Descriptions\n",
    "get_valid_addr= re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_addr_dict= {'state_code': [], 'state': []}\n",
    "with open('i94validaddr.txt') as f:\n",
    "    for line in f:\n",
    "        match= get_valid_addr.search(line)\n",
    "        valid_addr_dict['state_code'].append(match[1][:4])\n",
    "        valid_addr_dict['state'].append(match[2].lower())\n",
    "\n",
    "valid_addr= list(valid_addr_dict['state_code'])\n",
    "\n",
    "# Get valid dataframe\n",
    "def data_validation(valid_port, valid_cit, valid_addr, df):\n",
    "    #df_valid_port= df.filter(col('i94port').isin(valid_port))\n",
    "    df_valid= df.filter(\n",
    "        (col('i94port').isin(valid_port)) &\n",
    "        (col('i94cit').isin(valid_cit)) & \n",
    "        (col('i94res').isin(valid_cit)) &\n",
    "        (col('i94addr').isin(valid_addr))\n",
    "    )\n",
    "    return df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Filter out data that only show up in the valid list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_valid= data_validation(valid_port, valid_cit, valid_addr, imda_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Since cicid is the primary key for this dataset, it must be unique, hence drop all duplicate cicid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_dropdup= imda_df_valid.drop_duplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Check for missing and null values appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "missnull_check(imda_df_dropdup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can see that there are 3 columns (entdepu, occup, insnum) with over 98% missing or null values, we will drop these columns out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_columns= ['entdepu', 'occup', 'insnum']\n",
    "\n",
    "imda_df_dropcols= imda_df_dropdup.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* To handle with the 4th and 5th most missing or null column, visa post gender, we can consider replace missing or null values with **'Not mentioned'** to make the data more meaninful, for example, when query, the result will return F (female), M (male), and Not mentioned, that is much more meaning than just F, M, and Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_fillna= imda_df_dropcols.na.fill('Not mentioned', subset= ['gender', 'visapost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* About others collumn that have missing or null values, we perform a drop command to eliminate all of remain missing or null values where all columns is missing or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_final= imda_df_fillna.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Nextup, we convert the ```arrdate``` from SAS date form to readable datetime from and to create a dimension table later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sasdate_converter= udf(\n",
    "    lambda x: dt.datetime(1960, 1, 1) + dt.timedelta(days= int(x)),\n",
    "    TimestampType()\n",
    ")\n",
    "imda_df_final= (\n",
    "    imda_df_final\n",
    "    .withColumn('arrival_date', sasdate_converter('arrdate'))\n",
    "    .withColumn('arrival_year', year(sasdate_converter('arrdate')))\n",
    "    .withColumn('arrival_month', month(sasdate_converter('arrdate')))\n",
    "    .withColumn('arrival_day', dayofmonth(sasdate_converter('arrdate')))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f'Raw I94 Immigration dataset size: {imda_df.count()}')\n",
    "print(f'I94 Immigration dataset size after validate port, cit, and res: {imda_df_valid.count()}')\n",
    "print(f'I94 Immigration dataset size after handling missing and null values: {imda_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess Airport Code Dataset (function: preprocess_aircode_dataset())\n",
    "* Like the I94 Immigration dataset, we see that the column ident is the primary key, so we must check for its duplicate.\n",
    "* Drop all duplicate in the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_dropdup= ac_df.drop_duplicates(['ident'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* As we inspect the iso_region column, we break down the iso_region format into country and region and keep only the region value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define UDF for spliting iso_region column\n",
    "split_region= udf(\n",
    "    lambda x: x.split('-')[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_region= ac_df_dropdup.withColumn('iso_region', split_region('iso_region'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "missnull_check(ac_df_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Depend on the missing or null data lost ratio and consider its necessary to this project, these columns: 'iata_code', 'local_mode', 'gps_code' will be drop out this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_columns= ['iata_code', 'local_code', 'gps_code']\n",
    "\n",
    "ac_df_dropcols= ac_df_region.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Next, the since 'elevation_ft' is a numerical data and it has null and missing data, we can fill these data with the mean value of all available value in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "elevation_mean_df= ac_df_dropcols.select(mean(col('elevation_ft')).alias('elevation_mean')).collect()\n",
    "\n",
    "elevation_mean= elevation_mean_df[0]['elevation_mean']\n",
    "    \n",
    "ac_elevation_fillna= ac_df_dropcols.na.fill(elevation_mean, subset= ['elevation_ft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we drop all rows that all columns are missing or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ac_df_final= ac_elevation_fillna.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f'Raw Airport code dataset size: {ac_df.count()}')\n",
    "print(f'Airport code dataset size after handling missing and null values: {ac_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess US Cities Demographics (function: preprocess_us_cities_demographic_dataset())\n",
    "* With this dataset, we notice that there are 3 columns that can be considered to make a unique key, that is City. State and Race, we call it key column\n",
    "* First, we count all recorded in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we check for duplicated data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We see that there are not any duplicated values in the dataset, thus every record is bounded to the unique key (primary key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With each key column, we check distinct values for this columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.select('City').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.select('State').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.select('Race').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* With three key alone, we can not cover for all recorded in the dataset, we will considered combine these columns to create a composite key. In this case, we will combine all 3 keys, since we know that City is the main measurement here, and only State and Race can not cover all the record, so combine 3 column can create a composite key to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df.select(['City', 'State', 'Race']).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can see that 2891 unique records is match with the non-duplicated raw dataset --> We can conclude primary key for this dataset will be State-City-Race.\n",
    "* Next up, we will check for the missing or null value appear in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "missnull_check(uscd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The data loss ratio is below 0.5%, we will drop all row that included missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df_final= uscd_df.dropna(\n",
    "    subset=[\n",
    "        'Average Household Size', \n",
    "        'Number of Veterans', \n",
    "        'Foreign-born', \n",
    "        'Male Population',\n",
    "        'Female Population'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "uscd_df_final= uscd_df_final.withColumn('City', lower(col('City')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f'Raw US Cities Demographics dataset size: {uscd_df.count()}')\n",
    "print(f'US Cities Demographics dataset size after handling missing and null values: {uscd_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Preprocess World Temperature Data (function: preprocess_world_temperature_dataset())\n",
    "* Before deep dive into this dataset, we need to get back to the I94 Immigration dataset.\n",
    "* Get the year appear in the I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imda_df_final.select('i94yr').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We see that the I94 Immigration dataset only show the data for the year 2016. Thus, for this World Temperature dataset, we will filter only data for year 2016 only.\n",
    "* First, we break the dt column down into small portion like year, month, and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_ymd= (\n",
    "    wtd_df\n",
    "    .withColumn('record_year', year('dt'))\n",
    "    .withColumn('record_month', month('dt'))\n",
    "    .withColumn('record_day', dayofmonth('dt'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_ymd.select('record_year').\\\n",
    "distinct().\\\n",
    "orderBy('record_year', ascending= False).\\\n",
    "limit(5).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* As we can see, there is no record for the year 2016. The most recent year is 2013\n",
    "* Also, according to this [question](https://knowledge.udacity.com/questions/293780), we just can select most recent year in the dataset that as close as the year 2016\n",
    "* Thus, about this dataset, first we need to take only data where year from 2011 to 2013.\n",
    "* Also, as we aim to merge this dataset with the US Demographic dataset, so we only need temperature data from the US only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_filter= wtd_df_ymd.filter((col('record_year') >= 2011) & (col('Country')== 'United States')).drop('Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then we count all the record for this filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_filter.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* then, with the filtered data, we check for its duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_filter.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* We can confirm that there is no duplicated data appear in our dataset\n",
    "* Also, about this dataset, we can determine that two column dt and columnn City will the composite key to this dataset\n",
    "* Next, we check for the missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "missnull_check(wtd_df_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* This dataset is about temperature, and since there is only 1 record got missing or null values, it becames meaningless to use this record. Hence, we will drop this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_dropna= wtd_df_filter.dropna(\n",
    "    subset=[\n",
    "        'AverageTemperature', \n",
    "        'AverageTemperatureUncertainty'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Then, we lower all the city in the ```City``` column and calculate the mean temperature for both AverageTemperature and AverageTemperatureUncertainty columns by ```City```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "wtd_df_final= wtd_df_dropna.select([lower(col('City')).alias('us_city'), 'AverageTemperature', 'AverageTemperatureUncertainty'])\n",
    "wtd_df_final= wtd_df_final.groupBy('us_city').agg(round(avg('AverageTemperature'), 2).alias('avg_temperature'), round(avg('AverageTemperatureUncertainty'), 2).alias('avg_temperature_uncertainty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Finnally, let check the data size after the cleaning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(f'Raw Word Temperature dataset size: {wtd_df.count()}')\n",
    "print(f'Word Temperature dataset size after filtering, handling missing and null values: {wtd_df_dropna.count()}')\n",
    "print(f'Word Temperature dataset size after aggregated: {wtd_df_final.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### This is the end for Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "* For this project, the Data Model is designed as the following 1 fact table (**fact_immigration**) and 4 dimension tables (**dim_uscd**, **dim_aircode**, **dim_arrivaldate**)\n",
    "\n",
    "    ![Data model](data-model.png)\n",
    "\n",
    "\n",
    "1. **Fact Table:** **fact_immigration** This table data is extracted fromt the I94 Immigration dataset\n",
    "\n",
    "| fact_immigration |\n",
    "| --------- |\n",
    "| cic_id (PK) |\n",
    "| entry_year |\n",
    "| entry_month |\n",
    "| port_code |\n",
    "| arrival_sas_date |\n",
    "| entry_mode |\n",
    "| address_state_code |\n",
    "| origin_country_code |\n",
    "| origin_residential_country_code |\n",
    "| visa_mode |\n",
    "| count |\n",
    "| birth_year |\n",
    "| gender |\n",
    "| airline |\n",
    "| addmission_number |\n",
    "| flight_number |\n",
    "| visatype |\n",
    "\n",
    "2. **Dimension tables:**\n",
    "\n",
    "* **Aircode table**\n",
    "\n",
    "| dim_aircode |\n",
    "| --------- |\n",
    "| indent (PK) |\n",
    "| type |\n",
    "| name |\n",
    "| elevation_ft |\n",
    "| iso_country |\n",
    "| iso_region (FK) |\n",
    "| municipality |\n",
    "| coordinates |\n",
    "\n",
    "* **US Demographic table:** This table consists of two dataset: The US Demographics dataset and the World Temperature dataset\n",
    "\n",
    "| dim_uscd |\n",
    "| --------- |\n",
    "| city (CK) |\n",
    "| state (CK) |\n",
    "| race (CK) |\n",
    "| state_code (FK) |\n",
    "| median_age |\n",
    "| male_population |\n",
    "| female_population |\n",
    "| total_population |\n",
    "| number_veterans |\n",
    "| foreign_born |\n",
    "| avg_household_size |\n",
    "| count |\n",
    "| avg_temperature |\n",
    "| avg_temperature_uncertainty |\n",
    "| latitude |\n",
    "| longitude |\n",
    "\n",
    "* **Arrival date table:**\n",
    "\n",
    "| dim_arrivaldate |\n",
    "| --------- |\n",
    "| arrival_sas_date (PK) |\n",
    "| arrival_date |\n",
    "| arrival_year |\n",
    "| arrival_month |\n",
    "| arrival_day |\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The Data Pipeline step is as follow:\n",
    "1. Load 4 dataset from source: I94 Immigration dataset, Aircode dataset, US cities demographics dataset, and World Temperature dataset\n",
    "2. Clean the dataset and perform data aggregation to World Temperature dataset by attribute ```City``` (As perform in Step 2)\n",
    "3. Create fact table ```fact_immigration``` from I94 Immigration dataset and write to parquet file partitioned by ```entry_year``` and ```entry_month```\n",
    "4. Create dimension table ```dim_aircode``` from Aircode dataset and write to parquet file.\n",
    "5. Create dimension table ```dim_uscd``` from combined dataset US Citites Demographics and World Temperature, then write to partitioned by ```state```, ```city```, and ```race```\n",
    "6. Create two dimension tables dim ```dim_arrivaldate``` from I94 Immigration dataset's ```arrival_date```, then write to parquet file partitioned by year and month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "* Please refer to the **etl.py** script. In this script, we create a ETL Pipeline that operate from the beginning (loading datasource) to modeling data in the way that we can derive the requirement we set in the very first stage of this project. Note that in this script we have include function that has been declared and explained above (Step 1 and 2)\n",
    "* Run the ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> [INFO] STEP 0: Creating Spark Session...\n",
      " --> [SUCCESS] STEP 0: Spark Session created successfully!\n",
      " --> [INFO] STEP 1: Gathering data is in process...\n",
      " --> [SUCCESS] STEP 1 has been processed successfully !\n",
      " --> [INFO] STEP 2: Data Preprocessing is in process...\n",
      " --> [SUCCESS] STEP 2 has been processed successfully !\n",
      " --> [INFO] STEP 3: Data Modeling is in process...\n",
      " --> [SUCCESS] STEP 3 has been processed successfully !\n",
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- entry_year: integer (nullable = true)\n",
      " |-- entry_month: integer (nullable = true)\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- arrival_sas_date: integer (nullable = true)\n",
      " |-- entry_mode: integer (nullable = true)\n",
      " |-- address_state_code: string (nullable = true)\n",
      " |-- origin_country_code: integer (nullable = true)\n",
      " |-- origin_resident_country_code: integer (nullable = true)\n",
      " |-- visa_mode: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- addmission_number: integer (nullable = true)\n",
      " |-- flight_number: integer (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "Getting first 5 rows for fact_immigration_table\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_Pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f949c4edd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfact_immigration_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Getting first 5 rows for fact_immigration_table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfact_immigration_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_Pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Print dim_aircode_table schema and first 5 rows:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_Pandas'"
     ]
    }
   ],
   "source": [
    "fact_immigration_table, dim_aircode_table, dim_uscd_table, dim_arrivaldate_table= etl.ETL_Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. SQL check:  Query an SQL confirm that data model is right formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Rank the number of airport on every state list in the ```fact_immigration_table```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|state_code|airport_number|\n",
      "+----------+--------------+\n",
      "|        TX|          2277|\n",
      "|        CA|          1088|\n",
      "|        FL|           967|\n",
      "|        PA|           918|\n",
      "|        IL|           902|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct\n",
    "        inew.state_code as state_code,\n",
    "        count(a.name) as airport_number\n",
    "    from\n",
    "    (\n",
    "        select distinct\n",
    "            i.address_state_code as state_code\n",
    "        from\n",
    "            fact_immigration_temp as i\n",
    "    ) as inew\n",
    "    inner join\n",
    "        dim_aircode_temp as a\n",
    "    on\n",
    "        inew.state_code == a.iso_region\n",
    "    group by\n",
    "        state_code\n",
    "    order by\n",
    "        airport_number desc\n",
    "    limit\n",
    "        5\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Check the total population and average temperature in each state where immigrant move in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+--------------------+----------------+-------------------+\n",
      "|state_code|     state|       city|                race|total_population|average_temperature|\n",
      "+----------+----------+-----------+--------------------+----------------+-------------------+\n",
      "|        NY|  New York|   new york|American Indian a...|         8550405|              11.77|\n",
      "|        NY|  New York|   new york|               Asian|         8550405|              11.77|\n",
      "|        NY|  New York|   new york|               White|         8550405|              11.77|\n",
      "|        NY|  New York|   new york|Black or African-...|         8550405|              11.77|\n",
      "|        NY|  New York|   new york|  Hispanic or Latino|         8550405|              11.77|\n",
      "|        CA|California|los angeles|American Indian a...|         3971896|              16.93|\n",
      "|        CA|California|los angeles|Black or African-...|         3971896|              16.93|\n",
      "|        CA|California|los angeles|  Hispanic or Latino|         3971896|              16.93|\n",
      "|        CA|California|los angeles|               White|         3971896|              16.93|\n",
      "|        CA|California|los angeles|               Asian|         3971896|              16.93|\n",
      "|        IL|  Illinois|    chicago|               Asian|         2720556|               11.9|\n",
      "|        IL|  Illinois|    chicago|  Hispanic or Latino|         2720556|               11.9|\n",
      "|        IL|  Illinois|    chicago|               White|         2720556|               11.9|\n",
      "|        IL|  Illinois|    chicago|American Indian a...|         2720556|               11.9|\n",
      "|        IL|  Illinois|    chicago|Black or African-...|         2720556|               11.9|\n",
      "|        TX|     Texas|    houston|               Asian|         2298628|              21.91|\n",
      "|        TX|     Texas|    houston|American Indian a...|         2298628|              21.91|\n",
      "|        TX|     Texas|    houston|Black or African-...|         2298628|              21.91|\n",
      "|        TX|     Texas|    houston|               White|         2298628|              21.91|\n",
      "|        TX|     Texas|    houston|  Hispanic or Latino|         2298628|              21.91|\n",
      "+----------+----------+-----------+--------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct\n",
    "        i.address_state_code as state_code,\n",
    "        u.state as state,\n",
    "        u.city as city,\n",
    "        u.race as race,\n",
    "        u.total_population as total_population,\n",
    "        u.avg_temperature as average_temperature\n",
    "    from\n",
    "        fact_immigration_temp as i\n",
    "    inner join\n",
    "        dim_uscd_temp as u\n",
    "    on\n",
    "        i.address_state_code == u.state_code\n",
    "    order by\n",
    "        total_population desc\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. Check for null in primary key of each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_qualiity_check(quality_check_list, sqlcontext):\n",
    "    \n",
    "    failed_test_case= 0\n",
    "    \n",
    "    for sql in quality_check_list:\n",
    "        query= sql.get('check_sql')\n",
    "        expected_result= sql.get('expected_result')\n",
    "        \n",
    "        records= sqlcontext.sql(f'{query}').first()['count']\n",
    "        \n",
    "        if expected_result != records:\n",
    "            print(f'Failed at query {query}')\n",
    "            failed_test_case+= 1\n",
    "        \n",
    "    if failed_test_case > 0:\n",
    "        print(f'{failed_test_case} table(s) test failed')\n",
    "    else:\n",
    "        print('All tables has passed the data quality check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables has passed the data quality check\n"
     ]
    }
   ],
   "source": [
    "sqlcontext= SQLContext(spark)\n",
    "\n",
    "quality_check_list= [\n",
    "    {'check_sql': 'select count(*) as count from fact_immigration_temp where cic_id is null', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_aircode_temp where ident is null', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_uscd_temp where (city is null and state is null and race is null)', 'expected_result': 0},\n",
    "    {'check_sql': 'select count(*) as count from dim_arrivaldate_temp where arrival_sas_date is null', 'expected_result': 0}\n",
    "]\n",
    "\n",
    "data_qualiity_check(quality_check_list, sqlcontext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "1. fact_immigration\n",
    "    * ```cic_id```: entry id recorded immigrate into the US, stored as **Primary Key**\n",
    "    * ```entry_year```: entry year recorded immigrate into the US\n",
    "    * ```entry_month```: entry month recorded immigrate into the US\n",
    "    * ```port_code```: entry_port_code\n",
    "    * ```arrival_sas_date```: arrival data into the US, SAS format, stored as **Foreign Key**, link to ```dim_arrivaldate```\n",
    "    * ```entry_mode```: entry type when immigrate into the US (1 for Air, 2 for Sea, 3 for Land and 9 for Not Reported)\n",
    "    * ```address_state_code```: Immigrant address state code after immigrated into the US, stored as **Foreign Key**, link to ```dim_aircode``` and ```dim_uscd```\n",
    "    * ```origin_country_code```: Immigrant original country\n",
    "    * ```origin_resident_country_code```: Immigrant orginal residental country\n",
    "    * ```visa_mode```: Immigrant visa mode (1 for Business, 2 for Pleasure, and 3 for Student)\n",
    "    * ```count```: Immigrant number for one record\n",
    "    * ```birth_year```: Immigrant birth year (who fill the record)\n",
    "    * ```gender```: Immigrant gender (who fill the record)\n",
    "    * ```airline```: Airline that immigrant used when immigrate into the US\n",
    "    * ```addmission_number```: Record Admission Number\n",
    "    * ```flight_number```: Flight number of the Airline\n",
    "    * ```visatype```: Class of admission legally admitting the non-immigrant to temporarily stay in US\n",
    "    \n",
    "2. Dimension tables:\n",
    "* dim_aircode\n",
    "    * ```ident```: Airport id, stored as **Primary Key**\n",
    "    * ```type```: Airport type\n",
    "    * ```name```: Airport name\n",
    "    * ```elevation_ft```: Airport elevation\n",
    "    * ```iso_country```: Country that Airport located, ISO format \n",
    "    * ```iso_region```: Region (depend on country) that Airport located, ISO format, stored as **Foreign Key**, link to ```fact_immigration```\n",
    "    * ```municipality```: City, Town (depend on country, region) that Airport located\n",
    "    * ```coordinates```:  Airport coordinate\n",
    "    \n",
    "* dim_uscd\n",
    "    * ```state```: US state, stored as **Composite Key**\n",
    "    * ```city```: US city, stored as **Composite Key**\n",
    "    * ```race```: people race, stored as **Composite Key**\n",
    "    * ```state_code```: US state code, stored as **Foreign Key**, link to ```fact_immigration```\n",
    "    * ```median_age```: Population median age\n",
    "    * ```male_population```: Population of male\n",
    "    * ```female_population```: Population of female\n",
    "    * ```total_population```: Population for total gender\n",
    "    * ```number_veterans```:  Number of veterans in the population\n",
    "    * ```foreign_born```: Number of people not bornt in US\n",
    "    * ```avg_household_size```: Average Household size\n",
    "    * ```count```: Count of household\n",
    "    * ```avg_temperature```: Average temperature for each city, based on World Temperature dataset\n",
    "    * ```avg_temperature_uncertainty```: Average temperature uncertainty for each city, based on World Temperature dataset\n",
    "    * ```latitude```: City latitude\n",
    "    * ```longitude```: City longitude\n",
    "    \n",
    "* dim_arrivaldate\n",
    "    * ```arrival_sas_date```: Arrival date, SAS format, stored as **Primary Key**\n",
    "    * ```arrival_date```: Arrival date, Gregorian format\n",
    "    * ```arrival_year```: Arrival year\n",
    "    * ```arrival_month```: Arrival month\n",
    "    * ```arrival_day```: Arrival day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * For this project, we use **Apache Spark** since it suitable for batch processing and also with spark we can deal with large amount of data.\n",
    "    * Also, for the data model, we have designed the model as the **Star schema model**. The reasion behind this choice is that Star schema is the fundamental schema among the data mart schema and it is simplest. This schema is widely used to develop or build a data warehouse and dimensional data marts. It includes one or more fact tables indexing any number of dimensional tables. So that, not only the Data Engineer, but for Business Analyst or Data Scienctis, who is the user of the output of the ETL process, can catchup everything going on and easily comprehend the situation just only by looking at the data model.\n",
    "    \n",
    "* Propose how often the data should be updated and why.\n",
    "    * For this data, as fact table is immigration data, it is considered to update the data monthly since it is not necessary to see a day-by-day arrival/departure immigrants. This one-month update step will help the server can get a enough amount of data for each dataset to process.\n",
    "    * One more crucial factor is that when consider run an update any data, we must consider the update schedule of the source data, based on this article [here](https://www.trade.gov/i-94-arrivals-program), the source I94 Immigration data is update monthly, hence it is reccomended to update the data monthly\n",
    "    * Still, depend on the requirement of end-user, the update step is considered in order to fullfil both technical requirement and end-user requirement.\n",
    "    \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * We will consider to upload our pipeline to a better infrastructure, such as Amazon Web Service. The AWS infrastructure with S3 service will handle a vast amount of data in the case of our data get bigger. Redshift will create a cluster machine that can speed up the data processing.\n",
    "     \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * To update the data automatically, we will consider 2 options: AWS service Event Bridge or Apache Airflow. \n",
    "         * Event Bridge keep our update service always on time by trigger an update function. An update function can be a Python script for updating the data that deployed to a Lambda function or a Elastic Container Service (ECS).\n",
    "         * Apache Airflow is an service from Apache that automate an data pipeline through a Direct Acyclic Graph.\n",
    "     \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * With AWS, we can set an amount (can be over 100) user and its permission to any service by IAM service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
